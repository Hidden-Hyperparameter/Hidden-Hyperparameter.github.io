---
title: SQA-012
paper: "Consistency Model Made Easy"
paper_url: "https://arxiv.org/abs/2406.14548" 
paper_year: 2024
tags: 
    - Diffusion
    - "consistency models"
    - "ODE models"
layout: post
---

本文要解决的主要问题是 Consistency model 训练时间太长的问题. 提出了 __Easy Consistency Tuning__ 方法, 有效减少了计算消耗, 还能保证生成效果, 还保证了 scaling law. 看上去很吊打

## Formulation

consistency model $f(x_t, t)=x_0$ 等价于两个条件: $df/dt=0$ and $f(x_0, 0)=x_0$. (称作 __consistency condition__)

训练的时候使用 
$$
\frac{f_{\theta}(x_t)-f_{\theta}(x_r)}{t-r}
$$
来近似 $df/dt$. 一般的 loss 形式是
$$
\mathbb E_{x_0, \epsilon, t}w(t, r)\text d(f_{\theta}(x_t), f_{\text{sg}\theta}(x_r))
$$
with $w(t, r)=1/(t-r), x_t=x_0+t\epsilon, \epsilon\sim p(\epsilon)$

其中两次 forward 的 dropout seed 要一致.

## Problem

在 $N$ 大的时候, 误差会积累, 而 $N$ 小的时候不准. 这导致了很长的训练时间.

## ECT: Easy Consistency Tuning

核心路径是先 pretrain 一个 diffusion model, 然后再 tune 成 consistency model.

其中取的时间步骤 $\Delta t$ 一开始很大 (比如取 $t, 0$), 最后慢慢变小, 这样就能平滑的从 DM 目标到 CM 目标. __这有道理吗__。。

__Pros__: 即使 pretrain 一个 DM 也减少了很多 training cost

![image not found](/papers/SQA-012/ECT.png) 

## Design

### Mapping function: choose of time steps

文中把 $p(r|t)$ 叫做 __mapping function__. __注意这里我们设计对用一个 $t$ 唯一确定 $r$!__ 这有道理吗?

实际上, 我们还 condition on __epochs__. 注意到我们想要让训练后期 $r$ 接近 $t$, 我们的 parametrization 如下:

$$
\frac r t = 1-\frac 1 {q^a} n(t) = 1-\frac 1 {q^{[\text{iters}/d]}}n(t)
$$

然后让 $n(t)=1+k\sigma(-bt)=1+\dfrac{k}{1+e^{bt}}$. 一般来说我们取 $q>1, k=8, b=1$. 然后 clamp $r\ge 0$.

- 这样训练初期就有 $r/t=0$, 和 DM 一致.

### Choice of metric

作者认为 Psuedo-Huber loss 应该被看成是 $L_2$ loss 乘一个 __adaptive scaling factor__ $w(\Delta)$, 其中 $\Delta$ 是差异.

这样就直接取 $L_2$ 然后在 weight 中讨论.

### Weighting

Weighting function 很重要!

这里采用了两个 weighting: ($\Delta=f(x_t)-f(x_r)$)

$$
w(t)=\bar w(t)\cdot w(\Delta)=\bar w(t)\cdot\frac 1 {\sqrt{\|\Delta\|_2^2+c^2}}
$$

其中 $\bar w$ 叫做 __timestep weighting__, $w(\Delta)$ 叫做 __adaptive weighting__. (这个有深意的选取是有深意的)

## Results

![image not found](/papers/SQA-012/result.png) 

可以看到, training cost 和 Distillation 差不多, 但效果好多了!

![image not found](/papers/SQA-012/FID.png) 

发现 2-step FID 很好, 超越了 ICM! 但是 1-step 稍微差一点, 但已经很好. 训练的消耗可以达到 ICM 的 1/3 到 1/4.

- [Blog](https://gsunshine.notion.site/Consistency-Models-Made-Easy-954205c0b4a24c009f78719f43b419cc)

这个 Blog 中提到, 与其完全追求一步法生成, __不如把模型大小减半, 然后做两步生成__. 实际上这样确实效果更好!

## Scaling laws

在同一个模型做不同步数的 fine-tune. 

![image not found](/papers/SQA-012/scaling-law.png) 

呈现非常健康的 classic power law. 

## Limitations

本文提到, 这个 finetune 的方法是 dataset-dependent 的. 现在好像有些 Distillation 不需要数据集. __管__