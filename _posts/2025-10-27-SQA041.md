---
title: SQA-041
paper: "GPT-4 Technical Report"
paper_url: "https://arxiv.org/abs/2303.08774"
paper_year: 2023
tags: 
    - "LLM"
    - "Autoregressive"
    - "finetuning"
layout: post
---

这个报告以报结果为主，没有公开太多细节

整理:
1. GPT-4 完全基于 Transformer, 并使用 next token prediction pretrain
2. 预训练后使用 RLHF 做了 alignment
3. 模型可以接受 **图像+文本** 输入。但是没有公开技术细节。GPT-4o 进一步可以输出图像等模态

其它信息如下：
- 里面提到了如何使用小模型 ($<1/1000$) 的训练计算来预测最终模型的结果，包括 loss 和 evaluation score (近似)。也就是 scaling law
- 里面提到 post-training 对模型的推理能力/考试能力无显著影响，也就是还没有开始 RLVR
- Test-time techniques (CoT, few-shot prompting) 对多模态输入依然生效