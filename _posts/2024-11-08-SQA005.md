---
title: SQA-005
paper: "SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers"
paper_url: "https://arxiv.org/abs/2401.08740" 
paper_year: 2024
tags: 
    - "Flow Matching"
    - "Transformer"
    - "Diffusion"
    - "ODE models"
layout: post
---

卡多了没事干

## Interpolate between Diffusion and FM design space

就是做 ablations....

1. Continuous or Discrete: 连续的时间更好

2. Model parameterization: weighted score model (预测噪声) > 预测速度场 > score model

3. Interpolant: Linear < GVP ($\alpha_t=\cos(\pi t/2), \sigma_t=\sin(\pi t/2)$)

4. Noise in sampling? have noise is better

5. Tuning coefficients at inference time:
- 对于非线性的 interpolant, 使用
$$
w_t=w_t^{KL}=2(\dot{\sigma_t}\sigma_t-\frac{\dot{\alpha_t}\sigma_t^2}{\alpha_t})
$$
> Chosen to minimize an upper bound on the KL divergence $D_{KL}(p(x)\mid\mid p_0(x))$
- 对于线性的 interpolant, 使用
![](/papers/SQA-005/weight.png)

## Improve network architecture

> In order to eliminate any confounding factors andfocus on our exploration, we strictly follow the standard Diffusion Transformer (DiT) and its configurations. This way, we can also test the scalability of ourmodel across various model sizes.

方法: __Latent Diffusion models__, 即训练一个 autoencoder, 然后用 diffusion 模拟 latent space

具体细节见论文

## Classifier-free Guidance on velocity field

Simply use $v_{\theta}^{\zeta}(x, t;y)=\zeta v_{\theta}(x, t;y)+ (1-\zeta)v_{\theta}(x, t;\emptyset)$

正好能得到正确的分布