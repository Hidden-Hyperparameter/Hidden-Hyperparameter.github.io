---
title: JZC-011
paper: "The Impact of Initialization on LoRA Finetuning Dynamics"
paper_url: "https://arxiv.org/pdf/2406.08447" 
paper_year: 2024
tags: 
    - "LLM"
    - "finetuning"
layout: post
---

这篇paper比较了两种LoRA的初始化方法(Recall LoRA把$W$变成$W+BA$), 一种是把$A$初始化成0, $B$高斯初始化，
另一种相反，把$B$初始化成0, $A$高斯初始化。作者断言后者效果更好（也就是说一开始，考虑$x\to B(Ax)$ 的话，不要上来就把$x$变成0
，也很符合直觉)。作者从理论和实验上说明了这一点，并提出后者可以使用更高的学习率以及让训练更稳定。

实验结果上基本$A$高斯完全吊打$B$高斯

![image](/papers/JZC-imgs/11_1.png)
![image](/papers/JZC-imgs/11_2.png)