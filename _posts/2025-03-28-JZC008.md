---
title: JZC-008
paper: "S^4-Tuning: A Simple Cross-lingual Sub-network Tuning Method"
paper_url: "https://aclanthology.org/2022.acl-short.58.pdf" 
paper_year: 2022
tags: 
    - "Transformer"
    - "LLM"
    - "finetuning"
layout: post
---
(ACL 2022)
本文和![JZC-007](./2025-03-28-JZC007.md)类似，也是通过选择一个子网络进行针对性微调，特别地，对于每种语言，我们可以找到和它关系最大的参数集合，从而在这种语言上微调的时候，只对这些参数进行更新。这样的方法可以减少模型的参数量，提高模型的效率。作者在多个任务上进行了实验，证明了这种方法的有效性。

关于具体选择参数集的方法，非常简单粗暴，直接计算把每个参数变成0以后loss会影响多少，然后选择影响最大的那些参数。用数学公式表示就是:
$$\Omega(h_i) := \mathcal{L}(H, h_i) - \mathcal{L}(H, h_i = 0)$$
其中$H$是模型除了$h_i$以外的所有参数，$\mathcal{L}$是loss function。由于这个计算量还是很大，我们进一步近似成
$$\Omega(h_i) \approx \left| h_i \cdot \frac{\partial \mathcal{L}}{\partial h_i}\right|$$
从而通过梯度计算来选取参数集合。