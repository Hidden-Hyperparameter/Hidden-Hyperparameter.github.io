---
title: JZC-007
paper: "Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning"
paper_url: "https://arxiv.org/pdf/2109.05687" 
paper_year: 2021
tags: 
    - "Transformer"
    - "LLM"
    - "finetuning"
layout: post
---

(EMNLP 2021)

提出了一个（看上去比较显然）的finetune LLM的方法，是说在每次梯度更新的时候确定一个child network, 
只对这个小network进行梯度更新。然后对比了这个child network不同选法的表现，以及给了一些看上去很看上去的数学定理和证明。

伪代码如下:
![image](/papers/JZC-imgs/7_1.png)

child-network 大致有两种选法：一种是随机以某个比例选取(好好好)，以及另一个是针对固定的task,计算fisher information,然后更新关系比较大的那些，这样可以在保证大部分参数不变的情况下，只更新一部分参数，从而避免模型忘掉之前学到的东西。

- Random thoughts
    - 这种方法是否也可以用在CV上？（如果没有人做的话）