---
title: SQA-015
paper: "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
paper_url: "https://arxiv.org/abs/2501.12948" 
paper_year: 2025
tags: 
    - LLM
    - Transformer
layout: post
---

第一次看 LLM 的 paper

伟大无需多言

主要的贡献感觉是发现用纯 RL 可以把模型推理能力做 work 的很牛. 之前的 work 大部分都是在 train 之后用 RL fine-tune

# DeepSeek-R1-Zero

base model: DeepSeek-V3. employ GRPO (heavy RL training), with **no supervised data**.

## Group Relative Policy Optimization (GRPO)

对于老 policy $\pi_{\theta_{old}}$, 我们对 $\pi_{\theta}$ 对下面的目标最大化

![image not found](/papers/SQA-015/GRPO.png)
 
通俗理解, 就是对每道题, 生成多个回答, 然后把对的答案的概率提高, 错的答案的概率降低. 并且每次保证新策略不要变化太多.

这个 min 和 clip 比较迷, 我们分析一下:

对于 positive advantage: 当 $\dfrac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}$ 小于 $1+\epsilon$ 的时候会把他往大了优化, 否则没有梯度.

negative 类似. 很合理!

## Reward modeling

这里训练用的数据都是推理题目 (数学题), 所以有标答. 答案要求写在某个 box 内, 比如. reward 分成两类: 
1. Accuracy: 答案是否正确
2. Format: 格式是否正确

非常简单且合理

## Suprise: Aha moment

![image not found](/papers/SQA-015/aha.png)

模型自己学会了重新审视之前的推理. 很牛!

训练曲线很稳定, 有点离谱.

以及发现越训模型的思考越长.

此时的结果已经对齐 (有时超过) o1-0912 了! 使用 majority vote 能显著变好.

## Drawback

推理能力很强, 但是 readability 比较差, 思考的时候经常出现多种语言. 于是就训了一个 R1!

# DeepSeek-R1

大杂烩. 分为四个阶段.

1. *Cold start*.

为了防止最开始 RL 的不稳定 (它自己说的), 他们精心挑选了大概万条高质量 CoT data 来 fine-tune base model.

这些数据是 human-annotated 的. 这里提高了 readability.

2. *RL*.

使用 R1-Zero 同样的方法. 其中加入了 language consistency reward (这个 reward 仅 slightly 降低表现).

3. *Supervised fine-tuning*.

这里的数据分为几部分:
- **Reasoning data**: 从第二部后面的 checkpoint 生成的数据, 使用 rejection sampling, 只保留正确的. 其中去掉了混乱的 CoT 部分. (不一定看对了, 可能是我瞎看的)
- **Non-reasoning data**: 比如 writing, factual QA, self-cognition, translation. 使用 V3 生成的数据, 并且包含 CoT.

4. *RL for all senarios*.

这里包含很多部分:
- **reasoning (rule-based)**: 和 R1-Zero 一样
- **general data (human preference)**: 人类标注, 好像还说用 V3 作为 reward model 什么的
- **helpfulness**: 只关注最终结论, 对推理过程不干涉.
- **harmlessness**: 评估有害风险. 

# Distillation on smaller models

把 R1 直接 distill 到小模型, 比如 Qwen-32B, Llama-70B 什么的一大堆. 发现能够达成很高的性能!

实验发现, 如果对小模型直接应用 RL 训练, 虽然可以变好不少, 但是还不如直接从 R1 distill 过去.

总之很牛!