---
title: ZHH-010
paper: "Autoregressive Image Generation without Vector Quantization"
paper_url: "https://arxiv.org/abs/2406.11838"
paper_year: 2024
tags: 
    - Autoregressive
    - Transformer
    - Diffusion
layout: post
---

核心思想就是：根据前面的所有token $x_{<i}$ 生成一个latent $z$，然后condition on $z$ 来作一个diffusion来模拟 $x_i$ 的这个分布：

$$
p(x_i\mid x_{<i}) = p(x_i \mid z)
$$

实验发现这样的 **diff-loss** 比基于离散token的 **cross-entropy loss** 更好。另外一个好处是使用连续的token，可以把几个token合并成一个token，而减少sequence length（对于离散的就不行）。

然后，这篇论文还对Autoregressive的顺序进行研究：

- 可以按照raster order，也可以随机顺序；
- 可以一次生成一个token，也可以一次生成多个； 
- 可以使用不同的Transformer结构（causal会更快，但bidirectional会更好）。

实验发现，随机顺序、bidirectional更好；一次生成多个是更好的选择。