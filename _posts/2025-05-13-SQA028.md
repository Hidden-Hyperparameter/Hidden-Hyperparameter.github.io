---
title: SQA-028
paper: "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
paper_url: "https://arxiv.org/abs/1502.03167" 
paper_year: 2015
tags: 
    - "Optimization"
layout: post
---

BatchNorm 的原始 paper. 来看看它的故事.

# Problem

BatchNorm 是当时最早的 norm. 之前没有 norm 的时代, 使用的 lr 非常小否则会飞或者梯度消失. 所以训练很慢还不稳定. 

# Motivation

网络中前一层的移动会影响后面一层的输入的分布, 所以在训的时候后面的层需要适应前一层的分布变化. 这个被叫做 covariate shift. 

LeCun 说过, 对于一个 layer, 如果它的输入输出是 uncorrelated / whitened (mean 0, variance 1), 那么它的收敛会明显快. 结合这两点就有了 batchnorm 的动机 (method 我就不放了, 想必大家太不知道.)

# Important points in implementation

1. 训练时候计算得到的 mean 和 variance 需要放进计算图里面. 否则会有明显的反例: 比如前一层的梯度可能会让它越来越大导致爆掉. 具体例子我就不举了, 想不明白可以看图.

![image not found](/papers/SQA-028/computation_graph.png)

2. 一般来说默认 inference 的时候计算均值和方差会把整个数据集扫一遍计算. 为了训练过程中快速 log 才使用 moving average 的方法.

3. BN 一般的顺序是 Linear -> BN -> activation function. 此时前面一个 Linear 没有必要加 bias.

4. 对于卷积网络的 BN, 我们希望保持卷积的平移不变性, 也就是我们会希望每个位置的 shift 是一样的. 为此我们会看成是 $h\cdot w\cdot B$ 大小的 minibatch 做 BN.

# Benefits

1. 允许更大的 LR, 收敛基本上快 6 倍. 而且效果也提升.
2. 减少了对初始化的敏感性
3. BN 实际上是一种 regularization. 因为 batch stats 带来了随机性. 文中说部分时候可以取代 dropout 和 weight decay.