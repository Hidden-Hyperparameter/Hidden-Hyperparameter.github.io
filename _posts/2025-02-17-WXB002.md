---
title: WXB-002
paper: "One Step Diffusion via Shortcut Models"
paper_url: "https://arxiv.org/abs/2410.12557" 
paper_year: 2024
tags: 
    - Flow Matching
    - Distillation
layout: post
---

我们在 flow matching 学习的 score model $s_\theta(x,t)$ 的基础上，condition on **generate step length** $d$: $s_\theta(x_t,t,d)$, parameterization 是

$$
x_t+d\cdot s_\theta(x_t,t,d)\approx x_{t+d}.
$$

($x_0$ 是 noise, $x_1$ 是 data)

Objective:

$$
\mathcal L=\mathbb E_{(x_0,x_1)\sim (\mathcal N,\mathcal D),(t,d)\sim p(t,d)} \left[\|s_\theta(x_t,t,0)-(x_1-x_0)\|^2+\|s_\theta(x_t,t,2d)-s_{\text{target}}\|^2\right],
$$

其中

$$
s_{\text{target}}=\dfrac{1}{2}(s_{\theta}(x_t,t,d)+s_{\theta}(x_{t+d}',t+d,d)),
$$

$x_{t+d}'$ 是在 $t$ 处的预测结果 $x_t+d\cdot s_{\theta}(x_t,t,d)$. 第一项是基础的 **flow matching loss**, 第二项是 **self-consistency loss**.

![](/papers/WXB-002/suanfa.png)

### Details

- $t\sim \mathcal U(0,1)$; 论文中 sample step 是 $128$, $d$ 取值为离散的: $d\in \{\dfrac{1}{128},\dfrac{1}{64},\cdots,1\}$.
- 训练的时候，一个 batch 分为两部分, 分别 train flow matching loss 和 self-consistency loss；两者的比例是 $(1-k):k$, $k$ 取 $0.25$. 作者 claim 在训练过程中如果出现 self-consistency loss 不稳定的情况，就要减少 $k$.
- 推荐用 weight decay + $0.999$ 的 EMA.

### Experiments

步数减少到 1 步仍然能生成较好的图片; 且观察到了某种 scaling law? One-step generation 吊打了 consistency training (CelebAHQ-256 & Imagenet-256); 但是不如 two-stage 的 progressive distillation。

作者还测了 robotics tasks 上的表现。不愧是 Sergey Levine。

![](/papers/WXB-002/exp2.png)

![](/papers/WXB-002/exp.png)