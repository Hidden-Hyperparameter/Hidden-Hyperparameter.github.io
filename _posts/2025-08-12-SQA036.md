---
title: SQA-036
paper: "Diffuse and Disperse: Image Generation with Representation Regularization"
paper_url: "https://arxiv.org/abs/2506.09027"
paper_year: 2025
tags: 
    - "Diffusion"
    - "representation learning"
    - "contrastive learning"
layout: post
---

Runqian 老师的作品. 本文提出了加在 Diffusion Model 上面的一项附加 loss, 能让 diffusion 效果提升.

# Dispersive Loss as regularization

想法很简单，就是对网络中间某一层的 representation，加一个 loss 让它 spread out (两两远离)

这样的 loss 默认是
$$
\log\mathbb E_{i, j}[\exp(-\|z_i-z_j\|^2/\tau)]
$$
不知道为什么，这里的 $z$ 是不经过 normalize 的，理论上有飞的可能。

默认参数 $\tau=0.5$, Dispersive loss 的 weight 为 0.5. 默认选第三个 block

加到 SiT 上能从 2.06 降到 1.97，好像也不是很多

加到 Mean Flow 从 3.43 到 3.21

# Other loss

**InfoNCE loss**: 也就是上面那个，但是不一定是 $L_2$. 也可以用 cosine dissimilarity (但是效果不如 $L_2$)

**Hinge Loss**:
$$
\max(0, \epsilon-\mathcal D(z_i, z_j))
$$

**Covariance**: 计算 normalize 后的 $z$ 的 cross-correlation matrix ($d\times d$)

回顾 cross-correlation 的计算：对 $i, j\in [d]$
$$
C_{i, j} = \frac{\sum_{k\in B} z_{k, i}z_{k, j}}{\sqrt{\sum_{k\in B}z_{k, i}^2}\sqrt{\sum_{k\in B}z_{k, j}^2}}
$$
其快速计算的方法是，先把 $z$ 对一个 batch 做 normalize (指 batch norm, 减去整个 batch 里 $z$ 的均值再除以 std)，得到一个 $B\times d$ 的矩阵 $A$，然后做 $A^T A$.

$C$ 的对角线元素全为 1. 那么我们的 loss 就是最小化
$$
\sum_{i, j} C_{i, j}^2
$$

# Observation: Norm bigger

作者发现加了 dispersive loss 之后每一层的 hidden norm 都变大了一个数量级。唐

# Ablation: Contrastive loss

像是 representation learning 里面一样，每张图片取两个 view，然后把 dispersive loss 换成 contrastive loss.

发现，如果两个 view 加不同的噪声会变差。加同样的噪声能 work，但没有 dispersive 好