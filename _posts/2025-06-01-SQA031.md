---
title: SQA-031
paper: "Guiding a Diffusion Model with a Bad Version of Itself"
paper_url: "https://arxiv.org/abs/2406.02507"
paper_year: 2025
tags: 
    - "Diffusion"
    - "guidance"
layout: post
---

本文提出了 autoguidance, 又名 bad teacher, 说的是用一个更差的模型可以给好的模型做 guidance, 来起到比 cfg 更好的效果.

主要贡献是揭示了 class 并不是 guidance 的本质.

# Problem

practical 的 CFG 会导致 mode collapse 等问题.

Diffusion Model 由于等价的 KL 散度 loss, 会过分关注于 low prob 的训练样本, 为了避免这些点的 extreme penalty. 换句话说, 由于能力限制, 它会更倾向于 cover the entire training data. 但是这样, 就会导致一些低概率的 sample, 也就是不好的图片

![image not found](/papers/SQA-031/toy-1.png)

![image not found](/papers/SQA-031/toy-2.png)

CFG 的作用是能一定程度上消除这些 outliers, 但是如上面的实例, 会忽略掉很多 branch.

# Method

用两个模型, 都带 class label, $D_1$ 和一个更差的 $D_0$

我们期望/假设 $D_0, D_1$ 在 fitting 层面是在类似的地方 fit 的不好, 只不过是程度的区别 (similar error, only stronger)

这样的话, 让 $D_1$ 的 sample 远离 $D_0$ 就可以消除这样的误差 (low-prob regions)

## Ablations on error type

一个重要的点在于, 两个模型必须有 **same type of degradation**

![image not found](/papers/SQA-031/error-type.png)

注意这个只是 ablation, 正常不会这么干

## Auto-guidance choices

把 bad model 设置成 **under-trained & reduced capacity** 的模型

其中 less training 更能涨点. 默认采用 1/16 的 training

并且最优的地方, 两个模型使用的 ema 并不一样.

一般 reduce capacity 降一档就够了

![image not found](/papers/SQA-031/results.png)

对 ema 还挺敏感的 (m 代表 main model, g 代表 guiding model)

最后, 也探究了 guidance interval 的问题, 发现这个 guidance 对所有步骤都是有益的