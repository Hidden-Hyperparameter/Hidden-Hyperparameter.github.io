import os
from datetime import datetime
import re
import shutil
from pathlib import Path


# ANSI color codes for terminal output
class Colors:
    RED = '\033[91m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    RESET = '\033[0m'


def _print_warning(message: str):
    """Print warning message in red color"""
    print(f"{Colors.RED}Warning: {message}{Colors.RESET}")


def _print_success(message: str):
    """Print success message in green color"""
    print(f"{Colors.GREEN}{message}{Colors.RESET}")


def _print_info(message: str):
    """Print info message in blue color"""
    print(f"{Colors.BLUE}{message}{Colors.RESET}")


# åœ¨å®šä¹‰printå‡½æ•°åå†å¯¼å…¥arxiv
try:
    import arxiv
    ARXIV_AVAILABLE = True
except ImportError:
    ARXIV_AVAILABLE = False
    _print_warning("arxiv package not installed. Install with: pip install arxiv")


def get_arxiv_title(url: str) -> str:
    """
    ä»arxiv URLæˆ–IDè·å–è®ºæ–‡æ ‡é¢˜
    
    Args:
        url: arxiv URLæˆ–IDï¼Œå¦‚ "https://arxiv.org/abs/2501.03215" æˆ– "2501.03215"
        
    Returns:
        è®ºæ–‡æ ‡é¢˜ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
    """
    if not ARXIV_AVAILABLE:
        _print_warning("arxiv package not available. Cannot fetch title from arXiv.")
        return None
    
    try:
        # ä»URLæˆ–IDä¸­æå–arxiv ID
        id_pattern = r"(\d{4}\.\d{4,5})"
        match = re.search(id_pattern, url)
        
        if not match:
            _print_warning(f"Could not extract arxiv ID from: {url}")
            return None
        
        paper_id = match.group(1)
        _print_info(f"Fetching title for arxiv ID: {paper_id}")
        
        # è°ƒç”¨arxiv APIæŸ¥è¯¢
        search = arxiv.Search(id_list=[paper_id])
        paper = next(search.results(), None)
        
        if paper is None:
            _print_warning(f"Paper not found on arXiv: {paper_id}")
            return None
        
        # æ¸…æ´—æ ‡é¢˜ï¼ˆå»é™¤æ¢è¡Œç¬¦ï¼‰
        title = paper.title.replace('\n', ' ').strip()
        _print_success(f"Fetched title from arXiv: {title}")
        return title
        
    except Exception as e:
        _print_warning(f"Failed to fetch arxiv title: {e}")
        return None


def upload_paper(
    md_path: str,
    who: str,
    tags: list,
    paper_link: str = None,
    title: str = None,
    paper_year: str = None,
    time: str = None,
    who_count: str = None,
):
    """
    ä¸Šä¼ è®ºæ–‡ç¬”è®°åˆ°åšå®¢ä»“åº“
    
    Args:
        md_path: æºmarkdownæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        who: ç¼–è¾‘è€…æ ‡è¯†(å¦‚ "ZKY", "SQA" ç­‰)
        tags: æ ‡ç­¾åˆ—è¡¨
        paper_link: è®ºæ–‡é“¾æ¥(å¯é€‰ï¼Œæ”¯æŒarxiv URLè‡ªåŠ¨æå–paper_year)
        title: è®ºæ–‡æ ‡é¢˜(å¯é€‰ï¼Œä»md_pathæ–‡ä»¶åæˆ–paperæ ‡ç­¾æå–)
        paper_year: è®ºæ–‡å¹´ä»½(å¯é€‰ï¼Œå¦‚ä¸æä¾›ä¼šå°è¯•ä»arxiv URLæå–)
        time: å‘å¸ƒæ—¥æœŸ(å¯é€‰ï¼Œæ ¼å¼ä¸º YYYY-MM-DDï¼Œé»˜è®¤ä¸ºå½“å‰æ—¥æœŸ)
        who_count: ç¼–å·(å¯é€‰ï¼Œæ ¼å¼ä¸º"001"ç­‰ï¼Œä¸æä¾›åˆ™è‡ªåŠ¨é€’å¢)
    """
    
    # è·å–æºmarkdownæ–‡ä»¶å†…å®¹
    if not os.path.exists(md_path):
        raise FileNotFoundError(f"æºæ–‡ä»¶ä¸å­˜åœ¨: {md_path}")
    
    with open(md_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æå–paperæ ‡é¢˜ï¼ˆä»arxivã€markdown contentæˆ–æ–‡ä»¶åï¼‰
    paper_title = title
    if not paper_title:
        # 1. å°è¯•ä»arxiv URLè·å–æ ‡é¢˜ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        if paper_link:
            arxiv_title = get_arxiv_title(paper_link)
            if arxiv_title:
                paper_title = arxiv_title
        
        # 2. å¦‚æœarxivè·å–å¤±è´¥ï¼Œå°è¯•ä»markdown contentä¸­æå–
        if not paper_title:
            title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
            if title_match:
                paper_title = title_match.group(1).strip()
        
        # 3. æœ€åä»æ–‡ä»¶åæå–
        if not paper_title:
            paper_title = os.path.splitext(os.path.basename(md_path))[0]
    
    # å°è¯•ä»arxiv URLæå–paper_year
    if not paper_year and paper_link:
        try:
            extracted_year = extract_arxiv_year(paper_link)
            if extracted_year:
                paper_year = extracted_year
        except ValueError:
            pass  # å¦‚æœæå–å¤±è´¥åˆ™ç»§ç»­ï¼Œpaper_yearå¯ä»¥ä¸ºNone
    
    # è·å–å‘å¸ƒæ—¶é—´
    if time:
        publish_date = time
    else:
        publish_date = datetime.now().strftime("%Y-%m-%d")
        _print_info(f"No time provided, using current date: {publish_date}")
    
    # è·å–ä¸‹ä¸€ä¸ªç¼–å·
    posts_dir = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        "_posts"
    )
    
    # ç¡®å®šç¼–å·
    if who_count:
        # ä½¿ç”¨æ‰‹åŠ¨æŒ‡å®šçš„ç¼–å·
        number_str = who_count  # æ ¼å¼: "001"
        number = int(who_count)
    else:
        # è‡ªåŠ¨ç”Ÿæˆä¸‹ä¸€ä¸ªç¼–å·
        number = get_next_number(posts_dir, who)
        number_str = f"{number:03d}"
    
    # ç”Ÿæˆä¸¤ç§æ ¼å¼çš„ID
    post_id_filename = f"{who}{number_str}"  # ç”¨äºpostæ–‡ä»¶åï¼šZKY001
    post_id_display = f"{who}-{number_str}"  # ç”¨äºfrontmatterå’Œpapersç›®å½•ï¼šZKY-001
    
    # ç”Ÿæˆpostæ–‡ä»¶å
    post_filename = f"{publish_date}-{post_id_filename}.md"
    _print_info(f"Generated post filename: {post_filename}")
    
    post_path = os.path.join(posts_dir, post_filename)
    
    # å¤„ç†å›¾ç‰‡: æå–å¹¶å¤åˆ¶markdownä¸­çš„æœ¬åœ°å›¾ç‰‡
    repo_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    papers_dir = os.path.join(repo_root, "papers")
    post_images_dir = os.path.join(papers_dir, post_id_display)
    
    # å¤åˆ¶å›¾ç‰‡å¹¶æ›´æ–°å†…å®¹
    processed_content = copy_images(content, md_path, post_images_dir, post_id_display)
    # Replace pipes inside math blocks and validate overall format
    processed_content, format_errors = check_format(os.path.basename(md_path), processed_content)
    if format_errors:
        for e in format_errors:
            _print_warning(e)
        raise ValueError("Invalid '|' characters found in markdown. See warnings above.")
    
    # ç”Ÿæˆfrontmatter
    frontmatter = generate_frontmatter(
        title=post_id_display,
        paper=paper_title,
        paper_url=paper_link,
        paper_year=paper_year,
        tags=tags,
    )
    
    # åˆå¹¶frontmatterå’Œcontent
    full_content = frontmatter + "\n" + processed_content
    
    # ä¿å­˜åˆ°_postsç›®å½•
    os.makedirs(posts_dir, exist_ok=True)
    with open(post_path, 'w', encoding='utf-8') as f:
        f.write(full_content)
    
    _print_success(f"Successfully uploaded: {post_path}")
    return post_path


def get_next_number(posts_dir: str, who: str) -> int:
    """
    è·å–è¯¥ç¼–è¾‘è€…çš„ä¸‹ä¸€ä¸ªç¼–å·
    
    Args:
        posts_dir: _postsç›®å½•è·¯å¾„
        who: ç¼–è¾‘è€…æ ‡è¯†
        
    Returns:
        ä¸‹ä¸€ä¸ªç¼–å·ï¼ˆæ•´æ•°ï¼‰
    """
    if not os.path.exists(posts_dir):
        return 1
    
    # æŸ¥æ‰¾æ‰€æœ‰è¯¥ç¼–è¾‘è€…çš„postæ–‡ä»¶
    pattern = rf"\d{{4}}-\d{{2}}-\d{{2}}-{who}(\d{{3}})\.md"
    max_number = 0
    
    for filename in os.listdir(posts_dir):
        match = re.match(pattern, filename)
        if match:
            number = int(match.group(1))
            max_number = max(max_number, number)
    
    return max_number + 1


def generate_frontmatter(
    title: str,
    paper: str = None,
    paper_url: str = None,
    paper_year: str = None,
    tags: list = None,
) -> str:
    """
    ç”ŸæˆJekyll postçš„frontmatter
    
    Args:
        title: postæ ‡é¢˜
        paper: è®ºæ–‡æ ‡é¢˜
        paper_url: è®ºæ–‡é“¾æ¥
        paper_year: è®ºæ–‡å¹´ä»½
        tags: æ ‡ç­¾åˆ—è¡¨
        
    Returns:
        æ ¼å¼åŒ–çš„frontmatterå­—ç¬¦ä¸²
    """
    lines = ["---"]
    lines.append(f"title: {title}")
    
    if paper:
        lines.append(f'paper: "{paper}"')
    
    if paper_url:
        lines.append(f'paper_url: "{paper_url}"')
    
    if paper_year:
        lines.append(f"paper_year: {paper_year}")
    
    if tags:
        lines.append("tags:")
        for tag in tags:
            lines.append(f'    - "{tag}"')
    
    lines.append("layout: post")
    lines.append("---")
    
    return "\n".join(lines)


def extract_arxiv_year(paper_link: str) -> str:
    """
    ä»arxiv URLæå–è®ºæ–‡å¹´ä»½
    
    Args:
        paper_link: è®ºæ–‡é“¾æ¥ï¼Œå¦‚ "https://arxiv.org/pdf/2503.09641"
        
    Returns:
        å››ä½æ•°çš„å¹´ä»½å­—ç¬¦ä¸²ï¼Œå¦‚ "2025"ï¼Œå¦‚æœæ ¼å¼ä¸ç¬¦åˆåˆ™æŠ›å‡ºValueError
    """
    # åŒ¹é…arxiv IDæ ¼å¼: 25 (ä¸¤ä½æ•°è¡¨ç¤ºå¹´ä»½ï¼Œå¦‚ 25 = 2025)
    match = re.search(r'/(\d{4})\.(\d{5})', paper_link)
    if not match:
        raise ValueError(
            f"Invalid arxiv URL format: {paper_link}\n"
            f"Expected format like: https://arxiv.org/pdf/YYMM.XXXXX"
        )
    
    arxiv_id = match.group(1)
    # arxiv IDçš„å‰ä¸¤ä½æ˜¯å¹´ä»½ (20yymm.xxxxx æˆ– yymm.xxxxx)
    # ä»2007å¹´4æœˆå¼€å§‹ä½¿ç”¨æ–°æ ¼å¼ï¼Œå‰ä¸¤ä½ç›´æ¥è¡¨ç¤ºå¹´ä»½: 07 -> 2007, 25 -> 2025
    year_short = int(arxiv_id[:2])
    
    # è½¬æ¢ä¸ºå››ä½æ•°å¹´ä»½
    if year_short <= int(str(datetime.now().year)[-2:]):
        # å½“å‰å¹´ä»½çš„åä¸¤ä½æˆ–æ›´æ—©
        full_year = 2000 + year_short
    else:
        # å‡è®¾æ˜¯æ›´æ—©çš„å¹´ä»½ (å¤„ç†1990å¹´ä»£çš„æ–‡ç« )
        full_year = 1900 + year_short
    
    return str(full_year)


def copy_images(
    content: str,
    source_md_path: str,
    target_images_dir: str,
    post_id: str,
) -> str:
    """
    å¤åˆ¶markdownä¸­å¼•ç”¨çš„æœ¬åœ°å›¾ç‰‡åˆ°papersç›®å½•ï¼Œå¹¶æ›´æ–°markdownä¸­çš„å›¾ç‰‡è·¯å¾„
    
    Args:
        content: markdownå†…å®¹
        source_md_path: æºmarkdownæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        target_images_dir: ç›®æ ‡å›¾ç‰‡ç›®å½•è·¯å¾„ (å¦‚ /papers/ZKY001/)
        post_id: post ID (å¦‚ ZKY001)
        
    Returns:
        æ›´æ–°åçš„markdownå†…å®¹ï¼Œå›¾ç‰‡è·¯å¾„å·²æ”¹ä¸ºç»å¯¹äºpapersç›®å½•
    """
    source_dir = os.path.dirname(source_md_path)
    os.makedirs(target_images_dir, exist_ok=True)
    
    # åŒ¹é…markdownå’ŒHTMLä¸­çš„å›¾ç‰‡å¼•ç”¨
    # æ ¼å¼1: ![alt](path/to/image)
    # æ ¼å¼2: <img src="path/to/image" ...>
    
    updated_content = content
    
    # å¤„ç†markdownæ ¼å¼: ![alt](path)
    md_image_pattern = r'!\[([^\]]*)\]\(([^)]+?)(?:\s+"([^"]+)")?\)'
    
    def replace_md_image(match):
        alt_text = match.group(1)
        image_path = match.group(2)
        
        _print_info(f"Processing markdown image: alt='{alt_text}', path='{image_path}'")
        
        # åªå¤„ç†æœ¬åœ°å›¾ç‰‡ï¼ˆä¸ä»¥httpå¼€å¤´ï¼‰
        if image_path.startswith('http'):
            return match.group(0)
        
        new_path = copy_and_return_relative_path(
            image_path, source_dir, target_images_dir, post_id
        )
        return f"![{alt_text}]({new_path})"
    
    updated_content = re.sub(md_image_pattern, replace_md_image, updated_content)
    
    # å¤„ç†HTMLæ ¼å¼: <img src="path" ...>
    html_image_pattern = r'<img\s+src="([^"]+)"'
    
    def replace_html_image(match):
        image_path = match.group(1)
        
        _print_info(f"Processing HTML image: path='{image_path}'")
        
        # åªå¤„ç†æœ¬åœ°å›¾ç‰‡
        if image_path.startswith('http'):
            return match.group(0)
        
        new_path = copy_and_return_relative_path(
            image_path, source_dir, target_images_dir, post_id
        )
        return f'<img src="{new_path}"'
    
    updated_content = re.sub(html_image_pattern, replace_html_image, updated_content)
    
    return updated_content


def copy_and_return_relative_path(
    image_path: str,
    source_dir: str,
    target_dir: str,
    post_id: str,
) -> str:
    """
    å¤åˆ¶å•ä¸ªå›¾ç‰‡æ–‡ä»¶å¹¶è¿”å›ç»å¯¹è·¯å¾„
    
    Args:
        image_path: æºå›¾ç‰‡è·¯å¾„ï¼ˆå¯èƒ½æ˜¯ç›¸å¯¹æˆ–ç»å¯¹è·¯å¾„ï¼‰
        source_dir: markdownæ–‡ä»¶æ‰€åœ¨ç›®å½•
        target_dir: ç›®æ ‡å›¾ç‰‡ç›®å½•
        post_id: post ID
        
    Returns:
        ç»å¯¹äºåšå®¢æ ¹ç›®å½•çš„å›¾ç‰‡è·¯å¾„
    """
    # è§£ææºå›¾ç‰‡çš„å®Œæ•´è·¯å¾„
    if os.path.isabs(image_path):
        full_source_path = image_path
    else:
        full_source_path = os.path.normpath(
            os.path.join(source_dir, image_path)
        )
    
    _print_info(f"Resolving image path: '{image_path}'")
    _print_info(f"  source_dir: {source_dir}")
    _print_info(f"  resolved to: {full_source_path}")
    
    if not os.path.exists(full_source_path):
        _print_warning(f"Image not found: {full_source_path}")
        return image_path
    
    # è·å–æ–‡ä»¶å
    filename = os.path.basename(full_source_path)
    
    # å¤åˆ¶æ–‡ä»¶åˆ°target_dir
    target_path = os.path.join(target_dir, filename)
    shutil.copy2(full_source_path, target_path)
    _print_success(f"Copied image: {os.path.basename(full_source_path)} -> {target_path}")
    
    # è¿”å›ç»å¯¹äºåšå®¢æ ¹ç›®å½•çš„è·¯å¾„
    # æ ¼å¼: /papers/ZKY-001/image.png ï¼ˆpost_idåŒ…å«-)
    relative_path = f"/papers/{post_id}/{filename}"
    return relative_path


def check_format(title: str, content: str):
    """
    Ensure no stray '|' remain in markdown. Replace '|' inside $$...$$ with
    '\\mid' except when they are part of '\\left|' or '\\right|' or
    escaped '\\|'. Return (possibly_modified_content, errors_list).
    """
    errors = []

    def _replace_pipes_in_math(m):
        # ç°åœ¨ m.group(1) æ˜¯èµ·å§‹ç¬¦å· ($ æˆ– $$)
        # m.group(2) æ˜¯æ‹¬å·å†…éƒ¨çš„å…¬å¼å†…å®¹
        delim = m.group(1) 
        inner = m.group(2)
        
        # ä½ çš„åŸæœ‰ä¿æŠ¤é€»è¾‘ä¸å˜
        inner = inner.replace(r"\left|", "__LEFTPIPE__")
        inner = inner.replace(r"\right|", "__RIGHTPIPE__")
        inner = inner.replace(r"\|", "__ESCAPEDPIPE__")
        
        inner = inner.replace('|', r"\mid ")
        
        inner = inner.replace("__LEFTPIPE__", r"\left|")
        inner = inner.replace("__RIGHTPIPE__", r"\right|")
        inner = inner.replace("__ESCAPEDPIPE__", r"\|")
        
        # åŠ¨æ€è¿”å›ï¼šæ ¹æ®åŒ¹é…åˆ°çš„æ˜¯ä»€ä¹ˆè¾¹ç•Œï¼Œå°±è¿˜åŸä»€ä¹ˆè¾¹ç•Œ
        return f"{delim}{inner}{delim}"

    # æ­£åˆ™ä¿®æ”¹ï¼š(\${1,2}) åŒ¹é… $ æˆ– $$ å¹¶å­˜å…¥ group(1)
    # \1 ç¡®ä¿ç»“å°¾å’Œå¼€å¤´ä¸€è‡´
    content = re.sub(r"(\${1,2})(.*?)\1", _replace_pipes_in_math, content, flags=re.DOTALL)

    # For checking, remove explicit left/right/escaped pipes so they won't
    # be counted as errors
    check_content = content.replace(r"\left|", "").replace(r"\right|", "").replace(r"\\|", "")

    lines = check_content.split('\n')
    for i, line in enumerate(lines):
        if '|' in line and not (line.strip().startswith('|') and not line.strip().endswith('|')):
            # include surrounding context (2 lines before and after)
            start = max(0, i-2)
            end = min(len(lines), i+3)
            context_lines = []
            for ln in range(start, end):
                prefix = '>' if ln == i else ' '
                context_lines.append(f"{prefix} {ln+1}: {lines[ln]}")
            context_text = '\n'.join(context_lines)
            errors.append(
                f"ğŸ’©ğŸ’©FatalğŸ’©ğŸ’©: File {title}, line {i+1} contains invalid `|`. Context:\n{context_text}\nYou should use `|` properly, see README.md for more information"
            )

    return content, errors


def delete_paper(who: str, who_count: str = None, time: str = None):
    """
    åˆ é™¤å·²ä¸Šä¼ çš„è®ºæ–‡ç¬”è®°
    
    Args:
        who: ç¼–è¾‘è€…æ ‡è¯†(å¦‚ "ZKY", "SQA" ç­‰)
        who_count: ç¼–å·(å¦‚ "001", "002"ç­‰)ï¼Œå¦‚ä¸æä¾›åˆ™åˆ é™¤æœ€æ–°çš„
        time: æŒ‡å®šæ—¥æœŸ(æ ¼å¼"YYYY-MM-DD")ï¼Œç”¨äºç²¾ç¡®æŸ¥æ‰¾postæ–‡ä»¶
        
    Returns:
        è¢«åˆ é™¤çš„æ–‡ä»¶æ•°
    """
    repo_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    posts_dir = os.path.join(repo_root, "_posts")
    papers_dir = os.path.join(repo_root, "papers")
    
    # ç¡®å®šè¦åˆ é™¤çš„post ID
    if who_count:
        post_id_filename = f"{who}{who_count}"
        post_id_display = f"{who}-{who_count}"
    else:
        # æŸ¥æ‰¾è¯¥ç¼–è¾‘è€…çš„æœ€æ–°post
        latest_count = get_latest_number(posts_dir, who)
        if latest_count is None:
            _print_warning(f"No posts found for {who}")
            return 0
        post_id_filename = f"{who}{latest_count:03d}"
        post_id_display = f"{who}-{latest_count:03d}"
    
    # æŸ¥æ‰¾å¯¹åº”çš„postæ–‡ä»¶ï¼ˆä½¿ç”¨filenameæ ¼å¼ï¼‰
    post_file = None
    for filename in os.listdir(posts_dir):
        if post_id_filename in filename and filename.endswith('.md'):
            post_file = os.path.join(posts_dir, filename)
            break
    
    if not post_file:
        if time:
            # å°è¯•ä½¿ç”¨æ—¶é—´æˆ³æŸ¥æ‰¾
            pattern = f"{time}-{post_id_filename}.md"
            potential_file = os.path.join(posts_dir, pattern)
            if os.path.exists(potential_file):
                post_file = potential_file
        
        if not post_file:
            _print_warning(f"Post file not found for {post_id_filename}")
            return 0
    
    # åˆ é™¤postæ–‡ä»¶
    deleted_count = 0
    try:
        os.remove(post_file)
        _print_success(f"Deleted post: {post_file}")
        deleted_count += 1
    except Exception as e:
        _print_warning(f"Failed to delete post: {e}")
        return 0
    
    # åˆ é™¤å¯¹åº”çš„å›¾ç‰‡ç›®å½•ï¼ˆä½¿ç”¨displayæ ¼å¼ï¼‰
    images_dir = os.path.join(papers_dir, post_id_display)
    if os.path.exists(images_dir):
        try:
            shutil.rmtree(images_dir)
            _print_success(f"Deleted images directory: {images_dir}")
            deleted_count += 1
        except Exception as e:
            _print_warning(f"Failed to delete images directory: {e}")
    
    return deleted_count


def get_latest_number(posts_dir: str, who: str) -> int:
    """
    è·å–è¯¥ç¼–è¾‘è€…çš„æœ€æ–°ç¼–å·
    
    Args:
        posts_dir: _postsç›®å½•è·¯å¾„
        who: ç¼–è¾‘è€…æ ‡è¯†
        
    Returns:
        æœ€æ–°ç¼–å·ï¼ˆæ•´æ•°ï¼‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›None
    """
    if not os.path.exists(posts_dir):
        return None
    
    # æŸ¥æ‰¾æ‰€æœ‰è¯¥ç¼–è¾‘è€…çš„postæ–‡ä»¶
    pattern = rf"\d{{4}}-\d{{2}}-\d{{2}}-{who}(\d{{3}})\.md"
    max_number = None
    
    for filename in os.listdir(posts_dir):
        match = re.match(pattern, filename)
        if match:
            number = int(match.group(1))
            if max_number is None or number > max_number:
                max_number = number
    
    return max_number